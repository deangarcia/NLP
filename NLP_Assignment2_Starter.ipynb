{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment2_Starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deangarcia/NLP/blob/main/NLP_Assignment2_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdlZYgLKwBMk"
      },
      "source": [
        "In this assignment, you will familiarize yourself with:\n",
        "\n",
        "    spaCy\n",
        "    numpy\n",
        "    PyTorch\n",
        "\n",
        "to develop\n",
        "\n",
        "    Multiple sentiment analysis models\n",
        "\n",
        "to be able to\n",
        "\n",
        "    Predict whether a given review is positive or negative\n",
        "\n",
        "Before we begin, make sure your runtime is set to GPU -- \n",
        "\n",
        "Runtime > Change runtime type set to GPU\n",
        "\n",
        "First we will load in some data.\n",
        "\n",
        "Provided is code that will download a file and rename it to reviews.tar.gz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPUCX7P0x4c7"
      },
      "source": [
        "Included is a vocabulary of all the words we can potentially see -- No '\\<UNK\\>' *here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_1Hn4muw4Lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4cc555-8652-4143-997b-1b9674022ca4"
      },
      "source": [
        "!head aclImdb/imdb.vocab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n",
            "and\n",
            "a\n",
            "of\n",
            "to\n",
            "is\n",
            "it\n",
            "in\n",
            "i\n",
            "this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KAYND-Byigx"
      },
      "source": [
        "So, we have 12500 reviews for positive and negative classifications, and 50000 that are unlabeled.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtD78S4P0Rc9"
      },
      "source": [
        "Looking at the words in a review that don't show up in the vocab, we need to handle punctuation in a more delicate way (also, we will be making everything lower cased).  For this, we will turn to a standard modern NLP library spaCy.  spaCy is a library that handles a number of different low-level NLP tasks like tokenization, part-of-speech recognition, and named entity recognition.  For now, we will be focusing on the tokenization aspect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "positive_reviews = !ls -1 -d aclImdb/train/pos/*\n",
        "negative_reviews = !ls -1 -d aclImdb/train/neg/*\n",
        "unsupervised_reviews = !ls -1 -d aclImdb/train/unsup/*\n",
        "\n",
        "print(positive_reviews[:10])"
      ],
      "metadata": {
        "id": "mt4Q6RR2kprl",
        "outputId": "dceb94b7-0341-459e-ee14-944d600196b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-18 01:09:55--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.5’\n",
            "\n",
            "aclImdb_v1.tar.gz.5 100%[===================>]  80.23M  20.0MB/s    in 7.1s    \n",
            "\n",
            "2022-03-18 01:10:03 (11.2 MB/s) - ‘aclImdb_v1.tar.gz.5’ saved [84125825/84125825]\n",
            "\n",
            "['aclImdb/train/pos/0_9.txt', 'aclImdb/train/pos/10000_8.txt', 'aclImdb/train/pos/10001_10.txt', 'aclImdb/train/pos/10002_7.txt', 'aclImdb/train/pos/10003_8.txt', 'aclImdb/train/pos/10004_8.txt', 'aclImdb/train/pos/10005_7.txt', 'aclImdb/train/pos/10006_7.txt', 'aclImdb/train/pos/10007_7.txt', 'aclImdb/train/pos/10008_7.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "positive_reviews = !ls -1 -d aclImdb/train/pos/*\n",
        "negative_reviews = !ls -1 -d aclImdb/train/neg/*\n",
        "unsupervised_reviews = !ls -1 -d aclImdb/train/unsup/*\n",
        "\n",
        "vocabulary = set()\n",
        "array_review = []\n",
        "\n",
        "with open('./aclImdb/imdb.vocab') as vocab_file:\n",
        "  for word in vocab_file:\n",
        "    vocabulary.add(word.rstrip())\n",
        "\n",
        "with open(positive_reviews[0]) as review:\n",
        "  for line in review:\n",
        "    for word in line.rstrip().split(' '):\n",
        "      if word not in vocabulary:\n",
        "        pass\n",
        "        \n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a Tokenizer with the default settings for English\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "counts = {}\n",
        "rev_count = 0\n",
        "for review in positive_reviews + negative_reviews:\n",
        "  array_review.append(rev_count)\n",
        "  rev_count = rev_count + 1\n",
        "  with open(review) as review:\n",
        "    for line in review:\n",
        "      for word in tokenizer(line.rstrip().lower()):\n",
        "        if word.text not in vocabulary:\n",
        "          pass\n",
        "        else:\n",
        "          counts[word.text] = counts.get(word.text,0) + 1\n",
        "\n",
        "# The whole vocabulary needs to be setup so the words that appear most often are at the beggining of the set\n",
        "# so we need this extra array to organize our vocabulary\n",
        "count2words = {}\n",
        "for word in counts:\n",
        "  count = counts[word]\n",
        "  if count not in count2words:\n",
        "    count2words[count] = []\n",
        "  count2words[count].append(word)\n",
        "\n",
        "running_total = 0\n",
        "vocabulary = []\n",
        "for count in reversed(sorted(count2words)):\n",
        "  running_total += len(count2words[count])\n",
        "  vocabulary += count2words[count]\n",
        "  if running_total > 10000:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni7dUSj2KDy6",
        "outputId": "cc68565e-d23f-4034-ca32-cac23f8e897a"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-18 01:39:00--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.11’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  19.9MB/s    in 7.1s    \n",
            "\n",
            "2022-03-18 01:39:07 (11.3 MB/s) - ‘aclImdb_v1.tar.gz.11’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOXa3Gd2aTRA"
      },
      "source": [
        "We can now tokenize our text and the things we ignore are *mostly* puncutation (most tokenizers will split contractions like I'm into I and 'm which happens here, but the original vocab doesn't expect -- we will just let that happen here)\n",
        "\n",
        "\n",
        "#Step 1\n",
        "\n",
        "Fill out the function below to:\n",
        "\n",
        "* Tokenize a document\n",
        "* Extract all n-grams and their counts of the given order\n",
        "\n",
        "* It's ok to utilize your work from the first assignment here (although use SpaCy and go to lower case)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "\n",
        "def get_n_grams(filename,n):\n",
        "  ngrams = {}\n",
        "  texts = []\n",
        "  test = []\n",
        "\n",
        "  filex = open(filename,'r')\n",
        "  tokens = tokenizer(filex.read())\n",
        "  for token in tokens:\n",
        "    test.append(token.lemma_.lower())\n",
        "  texts.append(test)\n",
        "  #Text is a list of lists\n",
        "  for text in texts:\n",
        "    # iterate through each list (-n + 1 because of the nested for loop we dont want to go out of range and len(array) doesnt take into account 0)\n",
        "    for i in range(len(text) - n + 1):\n",
        "      temp = [] \n",
        "      # now iterate through the list j values which is the length of n to save our string values into a tuple\n",
        "      for j in range(n):\n",
        "        temp.append(text[i+j])\n",
        "      tuple_temp = tuple(temp)\n",
        "      # save the tuple as a key at increment it by one if it exists already \n",
        "      if tuple_temp in ngrams.keys():\n",
        "        ngrams[tuple_temp] += 1\n",
        "      # or set it to one on the first occurence\n",
        "      else:\n",
        "        ngrams[tuple_temp] = 1\n",
        "\n",
        "  return ngrams\n"
      ],
      "metadata": {
        "id": "ekL1ZkdZne1x"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRYAEI6EEsvz"
      },
      "source": [
        "We are going to use Stochastic Gradient Descent to learn the weights for our regression, and we will be utilizing the PyTorch library to do so"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2iPFoIij719"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nENMjMTDej3v"
      },
      "source": [
        "With the ability to extract out n-grams, we will now be constructing a linear classifier.  To do this, we will need to set up our data.  \n",
        "\n",
        "#Step 2\n",
        "\n",
        "* Make a $|D|\\times|unigrams|$ tensor (`torch.Tensor`) , `X_unigrams` that contains all of the unigram counts for the documents -- each row should be a document and each column should be a unigram -- each cell corresponding to the number of times the unigram corresponding to the column shows up in the document corresponding to the row  \n",
        "i.e., $X_{unigrams}[i,j] = $ Count of unigram $j$ in document $i$\n",
        "* Make a $|D|\\times1$ tensor, `Y` that contains the ratings for the documents -- we will say that a positive review has a rating of `1` and a negative review is `0`\n",
        "*In torch, if we want to use a GPU for the training we need to move the data to the GPU using `.to('cuda')`.  For this to work you will need to make sure you are using a GPU instance (Runtime > Change Runtime Type > Hardware Accelerator = GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAiUqVNwXb57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbc0a43-9293-489a-b01c-33d26bfea611"
      },
      "source": [
        "n = 1\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\") \n",
        "data = []\n",
        "ratings = []\n",
        "for filename in positive_reviews + negative_reviews:\n",
        "  substring = \"pos\"\n",
        "  rating_val = []\n",
        "  if substring in filename:\n",
        "    ratings.append(1)\n",
        "  else:\n",
        "    ratings.append(0)\n",
        "  row_data = []\n",
        "  gram = get_n_grams(filename, n)\n",
        "  for i in range(len(vocabulary)):\n",
        "    temp_list = []\n",
        "    for j in range(n):\n",
        "      temp_list.append(vocabulary[i])\n",
        "    cmp_tuple = tuple(temp_list)\n",
        "    if cmp_tuple in gram.keys():\n",
        "      row_data.append(gram[cmp_tuple])\n",
        "    else:\n",
        "      row_data.append(0)\n",
        "  data.append(row_data)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  X_unigrams = torch.FloatTensor(data).to(device)\n",
        "  y = torch.FloatTensor(ratings).to(device)\n",
        "  y = y.view(y.shape[0], 1)\n",
        "\n",
        "print(X_unigrams) # not sure if the data model is setup correctly here how would we do a bigram or trigram?\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9.,  1.,  4.,  ...,  0.,  0.,  0.],\n",
            "        [24.,  4., 17.,  ...,  0.,  0.,  0.],\n",
            "        [10.,  4.,  6.,  ...,  0.,  0.,  0.],\n",
            "        ...,\n",
            "        [12.,  8.,  7.,  ...,  0.,  0.,  0.],\n",
            "        [ 9.,  5., 12.,  ...,  0.,  0.,  0.],\n",
            "        [10.,  1.,  5.,  ...,  0.,  0.,  0.]], device='cuda:0')\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        ...,\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_unigrams))\n",
        "print(len(y))\n",
        "print(len(array_review))"
      ],
      "metadata": {
        "id": "TjbojQ6Nkff8",
        "outputId": "91624841-1a7e-4798-c2b4-8ff3c83781f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "25000\n",
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JecKqSs-JeYn"
      },
      "source": [
        "We will first construct a module that does what we want.  In torch you implement modules and you define the forward pass of the model (Torch uses autograd to automatically compute the backward pass based on the forward pass). \n",
        "\n",
        "#Step 3\n",
        "* Implement the linear regression using Torch\n",
        "* You will want to use a `torch.nn.Linear` layer -- The linear layer is essentially a matrix multiplication as shown in class  -- think about what the input dimension and output dimensions should be (how big is our input, how many things are we predicting)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAWNjtPfiFXz"
      },
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, size):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    self.linear = nn.Linear(size , 1).to(device)\n",
        "\n",
        "  def forward(self,X):\n",
        "    out = self.linear(X).to(device)\n",
        "    return out"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAMXe-LiK7uD"
      },
      "source": [
        "With the above model, we now need to actually train it.  We are going to use a pretty simple training regimen here.\n",
        "\n",
        "#Step 4 \n",
        "* Create the optimizer -- I recommend using the Adam optimizer (`optim.Adam`) -- optimizers take in the parameters that they are optimizing (`model.parameters`) and other hyperparameters like the learning rate (`lr=learning_rate`)\n",
        "* Loop for the number of `epochs` supplied to the training code -- this will be the number of passes over our data\n",
        "* In each epoch we will loop over the data in batches -- go from 0 up to the size of `X` in steps of size `batch_size`\n",
        "* Within each training step we will need to follow the steps of training a neural network:\n",
        " * Zero out the gradients in the optimizer (by default the gradients are kept around) by calling `.zero_grad()` on the optimizer\n",
        " * Get the current batch -- we can use *slicing* to get certain elements in our input and output.  E.g. `X[i:i+step_size,:]` would return a Tensor with elements from X from row i to row i+step_size and all of the columns\n",
        " * Run the model on the batch making sure to assign the result to a variable -- you can either call `.forward(...)` or more simply just call the model `(...)`\n",
        " * Calculate the loss of the output -- this is done by calling the `loss_criterion` with the predicted values and the true values as the first and second arguments respectively\n",
        " * Run the loss in the backward direction -- Call `.backward()` on the loss calcualted in the previous step\n",
        " * Step the optimizer -- Call `.step()` on the optimizer\n",
        " * You might want to do something like log the value of the loss -- this can be gotten by calling `.item()` on the loss calculated above -- perhaps do this every 5, 10, 50, 100, 500 epochs (your choice)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl_GpD5_iGPy"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def train(X, Y, model, batch_size, epochs, learning_rate, loss_criterion):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# dont count first array in the D x UNigram  tensor \n",
        "  for i in range(epochs + 1):\n",
        "    for step_size in range(batch_size):\n",
        "      optimizer.zero_grad()\n",
        "      output = model(X[i:i+step_size,:])\n",
        "      loss = loss_criterion(output, Y[i:i+step_size,:])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      #print(f'epoch {i} / {epochs}, step {step_size} / {batch_size}, loss = {loss.item():.4f}')\n"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16a5m2YnMoZu"
      },
      "source": [
        "#Step 5\n",
        "* Construct your model \n",
        "* Just as with the data above, if you want to use a GPU we have to tell torch to move the model (and parameters to the gpu) so you need to call `.to('cuda')` (Note this does not change the original model, it returns a model on the gpu so you probably want to do something like `model = model.to('cuda')`\n",
        "* Call the above training code -- watch how the loss changes as the model trains -- experiment with different training hyperparameters -- learning rate and epoch -- see when the model hits a minima\n",
        "* For a linear regression, we are using Mean Square Error as our loss `torch.nn.MSELoss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFStqz95MgqQ",
        "outputId": "fcf2f317-fb73-4992-fb74-ab0ae6b849ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "# Why is my first array empty\n",
        "  linear_model = LinearRegression(len(vocabulary)).to(device)\n",
        "\n",
        "  # How many times do we pass the training data over the over network\n",
        "  epochs = 2500\n",
        "\n",
        "  #If the data is to big to pass at once due to computer limitations pass it in\n",
        "  #smaller batches\n",
        "  batch_size = 1000\n",
        "  # Whenever you do learning and i goes over the learning rate if it reaches a minima and starts going up again reduce learning rate\n",
        "  learning_rate = 0.001\n",
        "  loss_criterion = nn.MSELoss()\n",
        "  train(X_unigrams, y, linear_model, batch_size, epochs, learning_rate, loss_criterion)\n",
        "  predicted = linear_model(X_unigrams).cpu().detach().numpy()\n",
        "  plt.plot(array_review[:5000], ratings[:5000])\n",
        "  plt.plot(array_review[:5000], predicted[:5000])\n",
        "  plt.show()"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-231-b6c86228ca0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mloss_criterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_unigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_review\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-228-49a5515abfa8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, model, batch_size, epochs, learning_rate, loss_criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-200-d0b945e3e518>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_W6f7iLQJV3"
      },
      "source": [
        "\n",
        "#Step 6\n",
        "* Implement the class below -- you should have two linear weights  (Input -> Hidden), (Hidden -> Output)\n",
        "* In your forward, make sure you apply a nonlinear activation function after going from (Input -> Hidden) before (Hidden -> Output) -- you can choose the nonlinear activation (sigmoid, tanh, relu, leaky-relu are all fine)\n",
        "* Train the model\n",
        "\n",
        "###Question 2\n",
        "How does this model compare to the above model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFL8_XzJQa52"
      },
      "source": [
        "class NonLinearRegression(nn.Module):\n",
        "  def __init__(self, size, hidden_size):\n",
        "    super(NonLinearRegression, self).__init__()\n",
        "    self.non_linear = nn.Linear(size , hidden_size).to(device)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.non_linear_2 = nn.Linear(hidden_size , 1).to(device)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,X):\n",
        "    out = self.non_linear(X).to(device)\n",
        "    out = self.relu(out).to(device)\n",
        "    out = self.non_linear_2(out).to(device)\n",
        "    out = self.sigmoid(out).to(device)\n",
        "    return out"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "hidden_size = ((len(vocabulary) * 2) / 3) + 1\n",
        "hidden_size = int(hidden_size)\n",
        "nonlinear_model = NonLinearRegression(len(vocabulary), hidden_size).to(device)\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 10 # what is the purpose of the batches?\n",
        "learning_rate = 0.00001 # what does this variable do?\n",
        "loss_criterion = nn.MSELoss()\n",
        "train(X_unigrams, y, nonlinear_model, batch_size, epochs, learning_rate, loss_criterion)"
      ],
      "metadata": {
        "id": "38pJbqVmN2BY",
        "outputId": "1de9dfa4-7413-429e-ac65-9cf58a9cad09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 / 5, step 1 / 10, loss = 0.2388\n",
            "epoch 1 / 5, step 2 / 10, loss = 0.2335\n",
            "epoch 1 / 5, step 3 / 10, loss = 0.2266\n",
            "epoch 1 / 5, step 4 / 10, loss = 0.2217\n",
            "epoch 1 / 5, step 5 / 10, loss = 0.2155\n",
            "epoch 1 / 5, step 6 / 10, loss = 0.2090\n",
            "epoch 1 / 5, step 7 / 10, loss = 0.2070\n",
            "epoch 1 / 5, step 8 / 10, loss = 0.1994\n",
            "epoch 1 / 5, step 9 / 10, loss = 0.1896\n",
            "epoch 1 / 5, step 10 / 10, loss = 0.1811\n",
            "epoch 2 / 5, step 1 / 10, loss = 0.1094\n",
            "epoch 2 / 5, step 2 / 10, loss = 0.1412\n",
            "epoch 2 / 5, step 3 / 10, loss = 0.1517\n",
            "epoch 2 / 5, step 4 / 10, loss = 0.1538\n",
            "epoch 2 / 5, step 5 / 10, loss = 0.1506\n",
            "epoch 2 / 5, step 6 / 10, loss = 0.1544\n",
            "epoch 2 / 5, step 7 / 10, loss = 0.1443\n",
            "epoch 2 / 5, step 8 / 10, loss = 0.1321\n",
            "epoch 2 / 5, step 9 / 10, loss = 0.1244\n",
            "epoch 2 / 5, step 10 / 10, loss = 0.1277\n",
            "epoch 3 / 5, step 1 / 10, loss = 0.1385\n",
            "epoch 3 / 5, step 2 / 10, loss = 0.1402\n",
            "epoch 3 / 5, step 3 / 10, loss = 0.1375\n",
            "epoch 3 / 5, step 4 / 10, loss = 0.1292\n",
            "epoch 3 / 5, step 5 / 10, loss = 0.1335\n",
            "epoch 3 / 5, step 6 / 10, loss = 0.1185\n",
            "epoch 3 / 5, step 7 / 10, loss = 0.1036\n",
            "epoch 3 / 5, step 8 / 10, loss = 0.0949\n",
            "epoch 3 / 5, step 9 / 10, loss = 0.1013\n",
            "epoch 3 / 5, step 10 / 10, loss = 0.0958\n",
            "epoch 4 / 5, step 1 / 10, loss = 0.1124\n",
            "epoch 4 / 5, step 2 / 10, loss = 0.1091\n",
            "epoch 4 / 5, step 3 / 10, loss = 0.0985\n",
            "epoch 4 / 5, step 4 / 10, loss = 0.1057\n",
            "epoch 4 / 5, step 5 / 10, loss = 0.0888\n",
            "epoch 4 / 5, step 6 / 10, loss = 0.0745\n",
            "epoch 4 / 5, step 7 / 10, loss = 0.0663\n",
            "epoch 4 / 5, step 8 / 10, loss = 0.0758\n",
            "epoch 4 / 5, step 9 / 10, loss = 0.0708\n",
            "epoch 4 / 5, step 10 / 10, loss = 0.0674\n",
            "epoch 5 / 5, step 1 / 10, loss = 0.0845\n",
            "epoch 5 / 5, step 2 / 10, loss = 0.0708\n",
            "epoch 5 / 5, step 3 / 10, loss = 0.0828\n",
            "epoch 5 / 5, step 4 / 10, loss = 0.0649\n",
            "epoch 5 / 5, step 5 / 10, loss = 0.0519\n",
            "epoch 5 / 5, step 6 / 10, loss = 0.0448\n",
            "epoch 5 / 5, step 7 / 10, loss = 0.0569\n",
            "epoch 5 / 5, step 8 / 10, loss = 0.0522\n",
            "epoch 5 / 5, step 9 / 10, loss = 0.0492\n",
            "epoch 5 / 5, step 10 / 10, loss = 0.0476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZRcvYJBaUtZ"
      },
      "source": [
        "Ok, we have a linear regression and a non-linear regression -- let's do a Logistic Regression and non-linear Logistic regression now:\n",
        "#Step 7\n",
        "* Implement a standard logistic regression and a logistic regression with a hidden layer and non-linear activation\n",
        "* These should be very similar to above except they should have `torch.Sigmoid` applied to the output\n",
        "* Note, `MSELoss` is no longer applicable here -- we need to use Binary Cross Entropy Loss `torch.nn.BCELoss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRHAMOm9a12l"
      },
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, size):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.logistic = nn.Linear(size, 1)\n",
        "\n",
        "  def forward(self,X):\n",
        "    out = torch.sigmoid(self.logistic(X))\n",
        "    return out\n",
        "\n",
        "\n",
        "class LogisticANN(nn.Module):\n",
        "  def __init__(self, size, hidden_size):\n",
        "    super(LogisticANN, self).__init__()\n",
        "    self.non_linear = nn.Linear(size , hidden_size).to(device)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.non_linear_2 = nn.Linear(hidden_size , 1).to(device)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,X):\n",
        "    out = self.non_linear(X).to(device)\n",
        "    out = self.relu(out).to(device)\n",
        "    out = self.non_linear_2(out).to(device)\n",
        "    out = self.sigmoid(out).to(device)\n",
        "    return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "logistic_reg_model = LogisticRegression(len(vocabulary)).to(device)\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 10 # what is the purpose of the batches?\n",
        "learning_rate = 0.01 # what does this variable do?\n",
        "loss_criterion = nn.BCELoss()\n",
        "train(X_unigrams, y, logistic_reg_model, batch_size, epochs, learning_rate, loss_criterion)\n",
        "\n",
        "hidden_size = ((len(vocabulary) * 2) / 3) + 1\n",
        "hidden_size = int(hidden_size)\n",
        "logisticann_model = LogisticANN(len(vocabulary), hidden_size).to(device)\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 10 # what is the purpose of the batches?\n",
        "learning_rate = 0.001 # what does this variable do?\n",
        "loss_criterion = nn.BCELoss()\n",
        "train(X_unigrams, y, logisticann_model, batch_size, epochs, learning_rate, loss_criterion)"
      ],
      "metadata": {
        "id": "ZFv4TB47Tp9j",
        "outputId": "5b7f46e5-55c1-46d2-e3b6-03f11e04d489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.6581, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.3180, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.1555, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.1036, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0684, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0397, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0339, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0162, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0077, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.7560, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.1621, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0672, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0263, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0071, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0014, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(4.4049e-05, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToziSarXRQAQ"
      },
      "source": [
        "# Step 8\n",
        "* This step is somewhat freeform -- just as it would be if you were doing exploratory resarch\n",
        "* Try doing some exploration of different models.  Some things you might try are:\n",
        " * different configurations of inputs (different order n-grams, combinations of n-grams)\n",
        " * different numbers and sizes of input layers\n",
        " * different non-linear activations\n",
        "\n",
        "* Normally, you should pick the model that does the best not on training data, but on the evaluation data.  Since this dataset doesn't come with an evaluation set, we will instead be using the test set as our evaluation set (this is obviously poor practice, and should not be done in the real world).  Or rather we will be using a portion of the test files as evaluation.\n",
        "\n",
        "* Pick a model architecture, train it, run it on the eval data \n",
        "* Do this multiple times, keep the best model \n",
        "\n",
        "#### Question 3:\n",
        "How do you pick the \"best\" model -- what metric do you want to use here?\n",
        "\n",
        "* As a note, there will be n-grams that show up in the evaluation and training sets that don't show up in the training data, you will have to be able to account for this (of course, ignoring is acceptable) but your code shouldn't fail\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5sC6wBUTJPG",
        "outputId": "79ca8359-5ef6-4860-863a-12e983669358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "positive_raw_test = !ls -1 -d aclImdb/test/pos/*\n",
        "negative_raw_test = !ls -1 -d aclImdb/test/neg/*\n",
        "\n",
        "positive_eval = positive_raw_test[:len(positive_raw_test)//4]\n",
        "negative_eval = negative_raw_test[:len(negative_raw_test)//4]\n",
        "positive_test = positive_raw_test[len(positive_raw_test)//4:]\n",
        "negative_test = negative_raw_test[len(negative_raw_test)//4:]\n",
        "\n",
        "\n",
        "print(len(positive_eval))\n",
        "print(len(negative_eval))\n",
        "print(len(positive_test))\n",
        "print(len(negative_test))\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3125\n",
            "3125\n",
            "9375\n",
            "9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QKSuebJTPZP",
        "outputId": "7dbdedf3-5c1f-43ee-8694-ee720808fc23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_correct = 0\n",
        "n_samples = 0\n",
        "n_test = 1\n",
        "for reviews_test in negative_reviews:\n",
        "  row_data_test = []\n",
        "  gram_test = get_n_grams(reviews_test, n)\n",
        "  for i in range(len(vocabulary)):\n",
        "    temp_list_test = []\n",
        "    for j in range(n):\n",
        "      temp_list_test.append(vocabulary[i])\n",
        "    cmp_tuple_test = tuple(temp_list_test)\n",
        "    if cmp_tuple_test in gram_test.keys():\n",
        "      row_data_test.append(gram_test[cmp_tuple_test])\n",
        "    else:\n",
        "      row_data_test.append(0)\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    test = torch.FloatTensor(row_data_test).to(device)\n",
        "  outputs_test = linear_model(test)\n",
        "  print(outputs_test)\n",
        "  _, predictions = torch.max(outputs_test, 0)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3983], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.4392], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5049], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6822], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1739], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3390], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6165], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9546], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6699], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2581], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0071], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9599], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4394], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4215], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8162], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3522], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3490], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6701], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2338], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6076], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4387], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.0465], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6434], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6350], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5149], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.3691], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7010], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7130], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8717], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.8156], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1684], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5156], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1432], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0969], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6044], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5189], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9912], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6954], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2189], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0246], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5080], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5235], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4389], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4599], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7532], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6692], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7465], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.9349], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8957], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6582], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4204], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3636], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7705], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4748], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5051], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.0541], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3669], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4931], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7454], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4630], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5521], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6042], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5072], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2317], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6746], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([3.3624], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5205], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3926], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3699], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4339], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.9539], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0768], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5911], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4955], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2945], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5154], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([3.9069], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5657], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1583], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2670], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9295], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6801], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8626], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7276], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2479], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6602], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9055], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3979], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4193], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2345], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5836], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4754], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5555], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5432], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2519], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6722], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4150], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3067], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3387], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0596], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.8878], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5474], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6175], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9350], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8171], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3895], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0817], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6679], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4223], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7395], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5052], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7246], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5022], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1341], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9889], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8035], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0754], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1517], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3908], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9646], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6851], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.8878], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9585], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0127], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9169], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3749], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7222], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5823], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6772], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6845], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4026], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9123], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1890], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5078], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7551], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5229], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2000], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6684], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5191], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3229], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6707], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6141], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4296], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5151], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2637], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0720], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0809], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4465], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7348], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0766], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6264], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7915], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6766], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5650], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6034], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6797], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6034], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4389], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0028], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.0833], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4618], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5361], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6596], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3583], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8221], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6276], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.3060], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9775], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5172], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0859], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4815], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4144], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7133], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5136], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1261], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5590], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7226], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6473], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.2408], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1890], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6924], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7983], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5172], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3893], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8297], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3852], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0212], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3864], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3675], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9058], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1015], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6674], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7442], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6528], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5453], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5442], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9977], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.3286], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3441], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5091], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4658], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4658], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4607], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5514], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4493], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6820], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4444], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5495], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6921], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2590], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9126], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5482], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4250], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1388], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2134], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4674], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8605], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7078], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6300], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5142], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2540], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2615], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3473], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1634], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3748], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1771], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5138], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3473], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7113], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7258], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6707], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5963], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8727], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7218], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3986], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3450], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8106], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2864], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1622], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5435], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5370], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6689], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2096], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7670], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1325], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7407], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4837], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.9684], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5516], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6331], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5641], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5851], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0497], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6204], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9574], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6350], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5074], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5913], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3458], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3902], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6421], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.8791], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4650], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4257], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1572], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5269], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4829], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3574], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5421], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6316], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9122], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0772], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7253], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7237], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4328], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8915], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3630], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7108], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6957], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3266], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4569], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2368], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5884], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3280], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1750], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1310], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4578], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5288], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4410], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6758], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3005], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9389], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7642], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2223], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6879], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3581], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6910], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6504], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3392], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5555], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4739], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5321], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1650], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0898], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0241], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0890], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5045], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4147], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6384], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6479], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8878], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6363], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6602], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3137], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6265], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3056], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.0064], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6413], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.1537], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5660], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4030], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4512], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6426], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4330], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4854], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4278], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4691], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9134], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4255], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3741], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6525], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.9051], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3674], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4063], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4781], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5030], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8145], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6861], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2320], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8524], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7602], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6907], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4813], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8917], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5819], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8940], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5449], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6078], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0608], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8761], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4961], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9118], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6118], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0099], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3530], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9722], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7136], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5033], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0482], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7664], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6866], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4582], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0524], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5380], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4577], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6165], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3724], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6083], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1495], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5502], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7789], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3647], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3755], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3346], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6341], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3449], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2732], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5343], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6846], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3539], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7064], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6256], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2459], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4030], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3018], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8781], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6100], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5322], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3265], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9170], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8129], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([3.0295], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6081], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4439], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2415], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9486], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9262], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3606], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4916], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5023], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8704], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8158], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8859], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6386], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5684], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4456], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2811], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4570], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8173], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6916], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4917], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9184], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6018], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9322], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2533], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6381], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4686], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3974], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9894], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2136], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6409], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2840], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4307], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9743], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2871], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6702], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4873], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1838], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7964], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8480], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6449], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6270], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1618], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8286], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3921], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4851], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.7261], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3923], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5032], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.7596], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3454], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9225], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.0784], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4013], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3167], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5026], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5675], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.7851], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4299], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7945], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2450], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3349], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5363], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3333], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7019], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6144], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5663], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8006], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7497], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6161], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2354], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5239], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6402], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1417], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5383], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2114], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3772], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5158], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.8203], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1064], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3932], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7951], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3949], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6854], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7139], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3327], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5877], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5558], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5472], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.8240], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6712], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4802], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4163], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6833], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8754], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3669], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2544], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2637], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4915], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4642], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7922], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.3801], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6362], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5252], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6328], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8396], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.2513], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7044], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2625], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7822], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([3.4427], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([2.4627], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6370], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9779], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.1589], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5472], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7033], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.4091], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6151], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9692], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5538], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.9107], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3786], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.1274], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4590], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.3036], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2210], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8751], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7253], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5414], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.5129], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6432], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5315], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5947], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.5369], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.7093], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4433], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6759], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7408], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2073], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2993], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.7566], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.2605], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4276], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([1.6689], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.6421], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4290], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.8298], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor([0.4616], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-ebb5eb84e7b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtemp_list_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mtemp_list_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcmp_tuple_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_list_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VpPC8pGUrUE"
      },
      "source": [
        "## Bonus: Augment with unsupervised data\n",
        "* The dataset comes with more unsupervised data than supervised -- come up with a way to augment your training set with the unsupervised data, such that your model performance improves (to do this comparison keep model architecture the same, and compare the augmented data model vs the original on the evaluation set)"
      ]
    }
  ]
}