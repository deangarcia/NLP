{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayesMultiLabelClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM85r8jzjvzcS9H//iAkc/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deangarcia/NLP/blob/main/NaiveBayesMultiLabelClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First what I need to do is find the probablity of the types of each poem so the ratio of love and all poems then the probability of Mythology and Folklore and probability of Nature. Then for each of those subsets I need to calculate the probability of each word occuring given that it is of a particular class. Than we see which proability is higher whichever is higher is the class.\n",
        "\n",
        "How do I take into account the title and author? should I just merge them? I could try both merging them and then also taking into account the probabilities."
      ],
      "metadata": {
        "id": "edXc5gk_cN76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_-6tLpvUe-A",
        "outputId": "151673e6-ed31-4ed1-920f-dd5e689c48fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "print(X.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "xCVH8gB-aKGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I have the vectorized counts of the words and all the contents, need to find the raw probability based off my examples from the merve dataset now I need to aggregate the author and the tile. Then take the counts and create another matrix for the probability value which is the counts over the total number of words in the vocab. Hard part with this is that the raw probability will have a huge weight on what the result will be. Could do unigram and bigram version of this too.\n",
        "\n"
      ],
      "metadata": {
        "id": "L6Ss87DHj9OA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "dataset = load_dataset(\"merve/poetry\")\n",
        "\n",
        "corpus = []\n",
        "for group in dataset:\n",
        "  for attributes in dataset:\n",
        "    #print(dataset[attributes])\n",
        "    for content in dataset[attributes]:\n",
        "      #print(content['content'])\n",
        "      corpus.append(content['content'])\n",
        "\n",
        "vectorizer = CountVectorizer() # what is the default tokenizer understand this more\n",
        "X = vectorizer.fit_transform(corpus) # figure out how this works \n",
        "vocab = vectorizer.get_feature_names()\n",
        "word_count_vectors = []\n",
        "#print(vocab)\n",
        "# go through the vocab and the remove any words that do not occur more than X amount of times lets say 3 replace with <UNK>\n",
        "# review this link to do this https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "#for counts in X.toarray()[150:155]:\n",
        "for counts in X.toarray():\n",
        "  word_count_vectors.append(list(np.asarray(counts) + 1))\n",
        "\n",
        "\n",
        "# Need to figure out how to make it so that the raw probability of it being a specific type doesnt overshadow the probabilities of the words occuring.\n",
        "#for counts in word_count_vectors:\n",
        "  #print(counts)\n"
      ],
      "metadata": {
        "id": "aEYnUDmvXZMA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}