{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 5170 -- HW 3 -- Word Vectors",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31a199edf0e44d54b2e87e45e3fdb116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9edee9ff837440cbad6416607586d1d1",
              "IPY_MODEL_84756f66ddfb49f08fa6c37d962acfd4",
              "IPY_MODEL_6ffe8b733aa64da0bf8a7f1af1f0004b"
            ],
            "layout": "IPY_MODEL_9a39662433e1424799eb7233022aa621"
          }
        },
        "9edee9ff837440cbad6416607586d1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff5d1a868b184589927a18f09a689601",
            "placeholder": "​",
            "style": "IPY_MODEL_c81f58b062454240873a741e441d12cb",
            "value": "100%"
          }
        },
        "84756f66ddfb49f08fa6c37d962acfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea65e00b8e73410ca0e8753ec758a0d2",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c05cf2faaff94fcb88fd2b7a884e6293",
            "value": 10000
          }
        },
        "6ffe8b733aa64da0bf8a7f1af1f0004b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b2fa936dacc40a6b548682dc62d33b2",
            "placeholder": "​",
            "style": "IPY_MODEL_df9f245301974dada8f7ce7baca0ca44",
            "value": " 10000/10000 [00:00&lt;00:00, 12843.46it/s]"
          }
        },
        "9a39662433e1424799eb7233022aa621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5d1a868b184589927a18f09a689601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c81f58b062454240873a741e441d12cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea65e00b8e73410ca0e8753ec758a0d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05cf2faaff94fcb88fd2b7a884e6293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b2fa936dacc40a6b548682dc62d33b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df9f245301974dada8f7ce7baca0ca44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deangarcia/NLP/blob/main/CS_5170_HW_3_Word_Vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you will:\n",
        "\n",
        "    Use Singular Value Decomposition (SVD) to compute word vectors\n",
        "    Use word2vec to compute word vectors\n",
        "    Compare the computed word vectors, qualitatively and quantitatively\n",
        "    Construct an analogical test for word vectors\n",
        "\n",
        "First, there is some code that will download a small subset of wikipedia."
      ],
      "metadata": {
        "id": "pbxx_dYi_9oO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vth0vaLdldon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee86ad24-2715-423a-cce7-501070d9d369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-30 22:54:03--  https://ndownloader.figshare.com/files/8768701\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 54.217.124.219, 52.16.102.173, 2a05:d018:1f4:d000:b283:27aa:b939:8ed4, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|54.217.124.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/8768701/TREx_json_sample.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20220330/eu-west-1/s3/aws4_request&X-Amz-Date=20220330T225404Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=d3f11e759fdfbc119bd54d4a32e433908991119a358265c3d8d32413ae692424 [following]\n",
            "--2022-03-30 22:54:04--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/8768701/TREx_json_sample.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20220330/eu-west-1/s3/aws4_request&X-Amz-Date=20220330T225404Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=d3f11e759fdfbc119bd54d4a32e433908991119a358265c3d8d32413ae692424\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.97.35\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.97.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22121605 (21M) [binary/octet-stream]\n",
            "Saving to: ‘8768701’\n",
            "\n",
            "8768701             100%[===================>]  21.10M  13.5MB/s    in 1.6s    \n",
            "\n",
            "2022-03-30 22:54:06 (13.5 MB/s) - ‘8768701’ saved [22121605/22121605]\n",
            "\n",
            "Archive:  8768701\n",
            "  inflating: re-nlg_0-10000.json     \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import itertools\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "import scipy.sparse.linalg\n",
        "import gensim\n",
        "from spacy.lang.en import English\n",
        "import gensim.models\n",
        "\n",
        "!wget https://ndownloader.figshare.com/files/8768701\n",
        "!unzip 8768701\n",
        "\n",
        "trex_json = json.load(open('re-nlg_0-10000.json' ,'r'))\n",
        "\n",
        "nlp = English()\n",
        "# Create a Tokenizer with the default settings for English\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "all_text = [[tok.text for tok in tokenizer(doc['text'].lower())] for doc in trex_json]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(['<UNK>'])\n",
        "word_count = 0\n",
        "\n",
        "for text in all_text:\n",
        "  vocabulary |= set(text)\n",
        "  word_count += len(text)\n",
        "    \n",
        "print('|D|', len(all_text))\n",
        "print('|V|', len(vocabulary))\n",
        "print('|W|', word_count)\n",
        "\n"
      ],
      "metadata": {
        "id": "0dciN617q6gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfe03f5-814b-4119-c89b-fd044d641348"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|D| 10000\n",
            "|V| 83259\n",
            "|W| 2012058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have 10000 documents consisting of a total of ~2,000,000 words.  Just as in the last homework, we will be truncating our vocabulary -- here we will remove all words that show up less than 4 times, leaving us with a vocabulary of ~24,000 words."
      ],
      "metadata": {
        "id": "JLUUIiVrWPse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = {}\n",
        "for text in tqdm(all_text):\n",
        "  for word in text:\n",
        "    counts[word] = counts.get(word,0) + 1\n",
        "\n",
        "for word in counts:\n",
        "  if counts[word] < 4:\n",
        "    vocabulary.remove(word)\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "id": "vwBNU99arMpv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "31a199edf0e44d54b2e87e45e3fdb116",
            "9edee9ff837440cbad6416607586d1d1",
            "84756f66ddfb49f08fa6c37d962acfd4",
            "6ffe8b733aa64da0bf8a7f1af1f0004b",
            "9a39662433e1424799eb7233022aa621",
            "ff5d1a868b184589927a18f09a689601",
            "c81f58b062454240873a741e441d12cb",
            "ea65e00b8e73410ca0e8753ec758a0d2",
            "c05cf2faaff94fcb88fd2b7a884e6293",
            "3b2fa936dacc40a6b548682dc62d33b2",
            "df9f245301974dada8f7ce7baca0ca44"
          ]
        },
        "outputId": "560fb557-bf36-4242-f841-55ad2b721c60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31a199edf0e44d54b2e87e45e3fdb116"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "*   Fill out the function `get_cooccurrences` -- it takes in the text of the files ( a list of lists of strings) and the window to consider for cooccurrences. A window of 1 would mean words that are next to each other are considered, 2 would include a skip of 1, etc.\n",
        "e.g., 'The black cat ran' -- a window of 1 would consider `('The','black'), ('black', 'cat'),('cat','ran')`, while a window of 2 would consist of the same:   `('The','black'), ('black', 'cat'),('cat','ran')` and  `('The','cat'), ('black', 'ran')`\n",
        "*   The function should return a dictionary with keys as pairs of words and their cooccurrence counts. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r6_M0nJfCNFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab2index = {v:i for i,v in enumerate(vocabulary)}\n",
        "\n",
        "def get_cooccurrences(texts,window):\n",
        "  cooccurrences = {}\n",
        "  for sentence in texts:\n",
        "    for i in range(len(sentence)):\n",
        "      for j in range(i, i+window):\n",
        "        for n in range(1,window):\n",
        "          if j + n < len(sentence):\n",
        "            if sentence[i] in vocab2index:\n",
        "              if sentence[j + n] in vocab2index:\n",
        "                temp_tup = tuple([sentence[i]] + [sentence[j + n]])\n",
        "                if temp_tup in cooccurrences:\n",
        "                  cooccurrences[temp_tup] += 1\n",
        "                else:\n",
        "                  cooccurrences[temp_tup] = 1\n",
        "  return cooccurrences\n",
        "\n",
        "# need to increment by i and i+1\n",
        "# then i and i+2\n",
        "# then i and i+3 when window = 3\n",
        "#cooccurrences = get_cooccurrences(all_text,4)\n"
      ],
      "metadata": {
        "id": "1ebCqYMxrm_y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#correct data struct\n",
        "vocab2index = {v:i for i,v in enumerate(vocabulary)}\n",
        "\n",
        "def get_cooccurrences(texts,window):\n",
        "  cooccurrences = {}\n",
        "  for sentence in texts:\n",
        "    for i in range(len(sentence)):\n",
        "      for j in range(1,window+1):\n",
        "        if i+j < len(sentence):\n",
        "          if sentence[i] in vocab2index:\n",
        "            if sentence[i+j] in vocab2index:\n",
        "              temp_tup = tuple([sentence[i]] + [sentence[i+j]])\n",
        "              if temp_tup in cooccurrences:\n",
        "                cooccurrences[temp_tup] += 1\n",
        "              else:\n",
        "                cooccurrences[temp_tup] = 1\n",
        "  return cooccurrences\n",
        "\n",
        "cooccurrences = get_cooccurrences(all_text,4)"
      ],
      "metadata": {
        "id": "edqJbd6BPxMW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pm3x1vuKo1rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1 TEST\n",
        "cooccurrences = get_cooccurrences([['the', 'black', 'cat', 'ran']], 2)\n",
        "print(vocab2index['black'])\n",
        "i = 0\n",
        "for con in cooccurrences.keys():\n",
        "  if i < 100:\n",
        "    #print(con[0])\n",
        "    print(con, cooccurrences[con])\n",
        "    i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbWynvZym6Tr",
        "outputId": "a4ee3bc3-d4f6-4d4c-a141-86ca356ce418"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6149\n",
            "('the', 'black') 1\n",
            "('the', 'cat') 1\n",
            "('black', 'cat') 1\n",
            "('black', 'ran') 1\n",
            "('cat', 'ran') 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "We need to turn this dictionary into a matrix.  As is, this matrix would be very, very large and very full of 0's.  We instead are going to construct a sparse matrix using the `scipy.sparse` library. Specifically, we are going to first construct a COOrdinate matrix (`scipy.sparse.coo_matrix`) passing in a tuple containing lists of values (the counts) and the coordinates (the vocab indices corresponding to the cooccurring words) \n",
        "\n",
        "\n",
        "*   Construct a list `data` containing all of the cooccurrence counts -- the i'th element in the list should correspond to the i'th elements in the other lists\n",
        "*   Construct lists `rows` and `cols` containing the coordinates (the vocab indices) corresponding to the words\n",
        "* Make sure these lists describe a symmetrical matrix (i.e. if we have `('hello','world'):5` then we also need ('world','hello'):5\n",
        "\n",
        "e.g.\n",
        "If we had a cooccurrence dictionary with `{('hello','world'):5,('goodbye','world'):2}` and `vocab2index = {'hello':0,'world':1, 'goodbye':2}` \n",
        "\n",
        "then we should have ` data = [5,5,2,2], rows = [0,1,1,2], cols = [1,0,2,1]` (ordering here only matters in that the i'th element across each should be consistent)\n",
        "\n"
      ],
      "metadata": {
        "id": "VoaK5aRyEFGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 2 Test\n",
        "cooccurrences = {}\n",
        "test_tup_one = tuple([\"black\"] + [\"world\"])\n",
        "test_tup_two = tuple([\"cat\"] + [\"world\"])\n",
        "cooccurrences[test_tup_one] = 5\n",
        "cooccurrences[test_tup_two] = 2"
      ],
      "metadata": {
        "id": "KGettiI4Etf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROW = 0\n",
        "COL = 1\n",
        "data = []\n",
        "rows = []\n",
        "cols = []\n",
        "i = 0\n",
        "for con in cooccurrences.keys():\n",
        "  data.append(cooccurrences[con])\n",
        "  rows.append(vocab2index[con[ROW]])\n",
        "  cols.append(vocab2index[con[COL]])\n",
        "  if i < 100:\n",
        "    i += 1\n",
        "    print(con)\n",
        "    print(cooccurrences[con], vocab2index[con[ROW]], vocab2index[con[COL]])\n",
        "\n",
        "cooccurrences = scipy.sparse.coo_matrix((data,(rows,cols)),shape=(len(vocab2index),len(vocab2index)))"
      ],
      "metadata": {
        "id": "MKO7R48sySn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3\n",
        "We now need to construct our word vectors using singular value decomposition -- `scipy.sparse.linalg.svds`\n",
        "\n",
        "* Compute the singular value decomposition of `cooccurrences` -- you will need to specify the dimensionality of the decomposition -- go with 100\n",
        "* Construct a dictionary with keys of the words that show up in the vocabulary and values corresponding to the 100 dimensional vectors"
      ],
      "metadata": {
        "id": "e08Zq0hzZqAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Sq6PuTd4ROrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "def get_svd_word_vectors(cooccurrences):\n",
        "  word_vectors = {}\n",
        "  cooccurrences = cooccurrences.asfptype()\n",
        "  U, s, V = svds(cooccurrences, 100)\n",
        "  for ind, word in enumerate(vocab2index):\n",
        "    word_vectors[(word,)] = U[ind]\n",
        "  return word_vectors\n",
        "\n",
        "svd_vecs = get_svd_word_vectors(cooccurrences)"
      ],
      "metadata": {
        "id": "2JFjWovX0zHJ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4\n",
        "Now, let's examine our word vectors.\n",
        "\n",
        "First, make a function that computes the `cosine_similarity` of two vectors.  Reminder that cosine similarity is defined as $\\frac{x \\cdot y}{||x||||y||}$"
      ],
      "metadata": {
        "id": "IQraDGlEaQB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def cosine_similarity(x,y):\n",
        "  return 1 - spatial.distance.cosine(x, y)\n",
        "\n"
      ],
      "metadata": {
        "id": "xxCo9QJQZKVe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cosine_similarity(svd_vecs[('cat',)], svd_vecs[('dog',)]))\n",
        "print(cosine_similarity(svd_vecs[('cat',)], svd_vecs[('black',)]))\n",
        "print(cosine_similarity(svd_vecs[('cat',)], svd_vecs[('cat',)]))\n",
        "print(cosine_similarity(svd_vecs[('cat',)], svd_vecs[('ran',)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6l7IgYdStCM",
        "outputId": "8ee7899e-0d75-4edd-870d-ee3c5062628b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4124856367792895\n",
            "0.1506685830502288\n",
            "1.0\n",
            "0.12377647200658293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5\n",
        "Now, let's make a function that given a word vector finds the top *k* most similar word vectors, in order of their similarity (most similar to least similar)\n",
        "\n",
        "This function should take in an optional list of words to ignore (their similarity will not be computed)."
      ],
      "metadata": {
        "id": "bV7Os60yasI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def get_k_closest(vector, word_vectors,k,ignored):\n",
        "  greatest = {}\n",
        "  for vector_comp in word_vectors.keys():\n",
        "    if word_vectors[ignored] is word_vectors[vector_comp]:\n",
        "      pass\n",
        "    else:\n",
        "      greatest[(ignored + vector_comp)] = cosine_similarity(vector, word_vectors[vector_comp])\n",
        "\n",
        "  od = {k: v for k, v in sorted(greatest.items(), key=lambda item: item[1])}\n",
        "  orl = []\n",
        "  top_k = {}\n",
        "  for x in list(reversed(list(od)))[0:k]:\n",
        "    orl.append({x, greatest[x]})\n",
        "  return orl\n",
        "\n",
        "for word in ['star','america','planet','constitution','belgium','dog','elephant']:\n",
        "  print(get_k_closest(svd_vecs[(word,)],svd_vecs,5,(word,)))"
      ],
      "metadata": {
        "id": "Xl2RQeb1nY2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f5af91-7612-4da6-a8eb-63a3b3ec3298"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{0.573135997648636, ('star', 'bistable')}, {0.5462317633182553, ('star', 'cpu')}, {0.5456331625299718, ('star', 'exhaustive')}, {0.5430530731227147, ('star', 'villa')}, {('star', 'satellites'), 0.5291030219164666}]\n",
            "[{0.7957758802826763, ('america', 'europe')}, {0.7314808499789585, ('america', 'china')}, {0.7142625768597732, ('america', 'australia')}, {0.7068308691797405, ('america', 'india')}, {('america', 'africa'), 0.6732892281444955}]\n",
            "[{0.8372169710799442, ('planet', 'earth')}, {0.8126582259836277, ('planet', 'band')}, {0.812321963760388, ('planet', 'zorn')}, {0.8066969512584167, ('planet', 'vh1')}, {0.805218954070743, ('planet', 'moon')}]\n",
            "[{0.6870855914626506, ('constitution', 'cruel')}, {0.6744578459342706, ('constitution', 'nationalism')}, {0.6666015496353921, ('constitution', 'inefficient')}, {0.665978928573311, ('constitution', 'advisory')}, {0.6603314942485786, ('constitution', 'speaker')}]\n",
            "[{0.7150267281873635, ('belgium', 'huey')}, {0.7063114684964562, ('belgium', 'ginger')}, {0.6907140041311952, ('belgium', 'horst')}, {('belgium', 'patience'), 0.6846863400859159}, {0.683478535272232, ('belgium', 'forestry')}]\n",
            "[{0.5966175439537579, ('dog', 'samuel')}, {0.5946251523694326, ('dog', 'tigin')}, {0.5921388530492008, ('dog', 'carter')}, {0.5815719416800176, ('dog', 'miller')}, {0.5756401393905123, ('dog', 'kul')}]\n",
            "[{0.8687241149853557, ('elephant', 'mulholland')}, {0.8623258676310324, ('elephant', 'monophosphate')}, {0.8514932569414467, ('elephant', 'fanny')}, {0.8489000973868245, ('elephant', 'triphosphate')}, {0.8434926074719474, ('elephant', 'repeaters')}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6\n",
        "\n",
        "We will now use a popular word vector library to compute word vectors using, Gensim.\n",
        "\n",
        "*  Compute Word Vectors using `gensim.models.Word2Vec` https://radimrehurek.com/gensim/models/word2vec.html\n",
        "* Make sure to use similar hyper-parameters as above -- don't include words that show up less than 4 times, have a window of size 5, compute 100 dimensional vectors"
      ],
      "metadata": {
        "id": "iOJTX530bw-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "#Given a trained Gensim Word2Vec Model this will extract the word vectors\n",
        "w2v_vecs = {word: model[word] for word in model.wv.index2word}"
      ],
      "metadata": {
        "id": "OPLGrIYoCUz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in ['star','america','planet','constitution','belgium','dog','elephant']:\n",
        "  print(get_k_closest(w2v_vecs[word],w2v_vecs,5,(word,)))"
      ],
      "metadata": {
        "id": "Q7UeCfdwNMtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "* How do the svd word vectors and word2vec vectors compare in terms of similarity?\n",
        "* Which would you find to make more sense?\n",
        "\n",
        "Moving on -- we will now test the words using a analogical test set."
      ],
      "metadata": {
        "id": "0wVgdwJCc3I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://download.tensorflow.org/data/questions-words.txt\n",
        "!head questions-words.txt"
      ],
      "metadata": {
        "id": "sYNUTVk2T4fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 \n",
        "* Go through the `questions-words.txt` file and construct a dictionary where the keys are the different kinds of analogies (denoted by lines that start with a `:` (e.g. `: capital-common-countries`) and values of lists of the questions falling under that kind of analogy -- the questions should be lists of lower-cased strings. (e.g. `'Athens Greece Havana Cuba'` -> `['athens','greece','havana','cuba']`)"
      ],
      "metadata": {
        "id": "zI5ChcosdP_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analogies = {}"
      ],
      "metadata": {
        "id": "LA3Z8gRopGbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8\n",
        "*  Perform the vector math for computing an analogy in vector space.  This should return a vector corresponding to 'D' given 'A is to B as C is to D'\n",
        "* Combine everything up to this point to assess how the above word vectors perform in this analogical reasoning\n",
        "  *  For each analogy in the test set, compute the vector corresponding to the final entry\n",
        "  * Use this computed vector to find the top 5 most similar words found in the dictionary of word vectors, using the A, B, and C words as ignored words\n",
        "  * Compute the accuracy of the word vectors scoring a positive example as the desired word appearing in the top 5 examples, and a negative as otherwise\n",
        "  * Return a dictionary with the overall accuracy, as well as the per-category accuracies \n"
      ],
      "metadata": {
        "id": "e_beX8EmeBQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute A is to B as C is to ???\n",
        "def compute_analogy(A,B,C):\n",
        "  return None\n",
        "\n",
        "def score_analogies(vecs, analogies):\n",
        "  return {}\n",
        "\n",
        "for kind, word_vectors in [('SVD',svd_vecs), ('W2V',w2v_vecs)]:\n",
        "  print(kind)\n",
        "  for category, accuracy in score_analogies(word_vectors,analogies):\n",
        "    print(category, accuracy)\n",
        "  print('')"
      ],
      "metadata": {
        "id": "1uZnTW-xpOcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 9\n",
        "* Construct a new kind of analogical reasoning test -- construct 10 examples for this analogical reasoning.  Again, compare the above word vector approaches on your test.\n",
        "\n",
        "# Question 2\n",
        "* What did you intend to test with your analogical reasoning?  \n",
        "* How did the word vectors do? "
      ],
      "metadata": {
        "id": "4TlIuoBXhxox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_analogies = {'Your Analogies':[]}\n",
        "\n",
        "for kind, word_vectors in [('SVD',svd_vecs), ('W2V',w2v_vecs)]:\n",
        "  print(kind)\n",
        "  for category, accuracy in score_analogies(word_vectors,your_analogies):\n",
        "    print(category, accuracy)\n",
        "  print('')"
      ],
      "metadata": {
        "id": "n0sOWyteiXJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10\n",
        "* Once again, we will open this up.  Gensim comes with a number of precomputed word vectors.  Try a couple and see how they perform on the above analogical reasoning tests (both the existing and yours).  Compare and constrast their results.  \n",
        "* Some options:\n",
        "    * Compare different approaches (fasttext vs word2vec vs glove)\n",
        "    * Compare different dimensionalities (50d vs 100d vs 200d)\n",
        "    * Compare different datasets (Gigaword vs Twitter)"
      ],
      "metadata": {
        "id": "bXwcEAyIie6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "# Show all available models in gensim-data\n",
        "print('\\n'.join(list(gensim.downloader.info()['models'].keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX322fEy1BYb",
        "outputId": "1cfa60ee-aed0-4b15-eece-51c95c112f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fasttext-wiki-news-subwords-300\n",
            "conceptnet-numberbatch-17-06-300\n",
            "word2vec-ruscorpora-300\n",
            "word2vec-google-news-300\n",
            "glove-wiki-gigaword-50\n",
            "glove-wiki-gigaword-100\n",
            "glove-wiki-gigaword-200\n",
            "glove-wiki-gigaword-300\n",
            "glove-twitter-25\n",
            "glove-twitter-50\n",
            "glove-twitter-100\n",
            "glove-twitter-200\n",
            "__testing_word2vec-matrix-synopsis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O3ejAo7dp44o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}