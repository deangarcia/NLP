{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment1_Starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deangarcia/NLP/blob/main/NLP_Assignment1_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoLhgpq8A37v"
      },
      "source": [
        "In this assignment, you will familiarize yourself with:\n",
        "\n",
        "\n",
        "\n",
        "*   Python\n",
        "*   Jupyter Notebooks\n",
        "*   Google Colab\n",
        "\n",
        "to develop\n",
        "\n",
        "* A n-gram based language model, with smoothing\n",
        "\n",
        "to be able to\n",
        "\n",
        "* Produce naturalish text\n",
        "* Rate the perplexity of a text given your model\n",
        "\n",
        "First we will load in some data.\n",
        "\n",
        "Provided is code that will download a file and rename it corpus.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9W83a-6Az4K"
      },
      "source": [
        "!wget -O corpus.zip https://digitalrepository.wheatoncollege.edu/bitstream/handle/11040/24451/Shakespeare%20Corpus%20by%20Play.zip?sequence=1&isAllowed=y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtEkrsKIcfos"
      },
      "source": [
        "By typing \"!\" in a notebook, you can use command line prompts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaqV2dT5C9T5"
      },
      "source": [
        "!ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Ku2ezMBmlm"
      },
      "source": [
        "\n",
        "Now, we will unzip the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19CEka8eDKpq"
      },
      "source": [
        "!unzip -o corpus.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq8mNvQ_D07b"
      },
      "source": [
        "We can even have python and the command line interact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeRC0gLPDPsC"
      },
      "source": [
        "files = !ls Shake*/*/*_noSCRB.txt \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q-QJF9_D7FR"
      },
      "source": [
        "`files` is now a list containing all of the files we wish to use in our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPcAzaBDTei"
      },
      "source": [
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koxj6yTaEkPt"
      },
      "source": [
        "Note, there are extra quotes on the files, so we will use list comprehension to remove those"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Elxvnp8ExLs"
      },
      "source": [
        "files = [file[1:-1] for file in files]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQRR6E_ERL3"
      },
      "source": [
        "#STEP 1\n",
        "\n",
        "\n",
        "* Go through all of the files\n",
        "  * Load the file `file = open(filename,'r')` \n",
        "  * Use the spacy tokenizer to tokenize the text `tokens = tokenizer(file.read())`\n",
        "  * Loop through the tokens and make everything lowercase `.lower()`\n",
        "  * Use all of this to make a list (for each file) of lists of strings (the tokens in each file) called `texts` \n",
        "\n",
        "\n",
        "\n",
        "  e.g.\n",
        "\n",
        "  If we read in one file that looked like:\n",
        "  \n",
        "    HAMLET: Alas poor Yorick.\n",
        "\n",
        "    GRAVEDIGGER: Put that skull down.\n",
        "\n",
        "\n",
        "\n",
        "  The end results should look like:\n",
        "\n",
        "  `texts = [['<s>', 'HAMLET:', 'Alas', 'poor', 'Yorick.','\\n','GRAVEDIGGER:', Put','that', 'skull', 'down.', '</s>']]`\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "1ohoxRLsyKfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDQWXoeNEE5M"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
        "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
        "suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
        "infix_re = re.compile(r'''[-~]''')\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
        "                                prefix_search=prefix_re.search,\n",
        "                                suffix_search=suffix_re.search,\n",
        "                                infix_finditer=infix_re.finditer)\n",
        "    \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "tokenizer = nlp.tokenizer\n",
        "texts = []\n",
        "\n",
        "for filename in files:\n",
        "  test = []\n",
        "  filex = open(filename,'r')\n",
        "  tokens = tokenizer(filex.read())\n",
        "  for token in tokens:\n",
        "    test.append(token.lemma_.lower())\n",
        "  texts.append(test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4jiUQ9QHjRX"
      },
      "source": [
        "#STEP 2\n",
        "\n",
        "With our files read in, it's time to do some counting!\n",
        "\n",
        "* Implement the function `get_counts(texts,n)` which takes in a list of list of strings -- `texts` -- and the arity of the n-gram -- `n` \n",
        "* `get_counts` should return a dictionary where the keys are the n-grams (as tuples of strings) and the values are the integer counts of all of the found n-grams\n",
        "\n",
        "\n",
        "Here's a quick primer on tuples in Python\n",
        "\n",
        "    triple = (1,2,3)\n",
        "    double = (1,2)\n",
        "    single = (1,) #note the trailing comma to distinguish from parenethesis \n",
        "    null_tuple = tuple() \n",
        "\n",
        "    list_triple = [1,2,3]\n",
        "    tuple_triple = tuple(list_triple)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx-MSkDlEeW6"
      },
      "source": [
        "def get_counts(texts,n):\n",
        "  ngrams = {}\n",
        "  for text in texts:\n",
        "    for i in range(len(text) - n + 1):\n",
        "      temp = [] \n",
        "      for j in range(n):\n",
        "        temp.append(text[i+j])\n",
        "      tuple_temp = tuple(temp)\n",
        "      if tuple_temp in ngrams.keys():\n",
        "        ngrams[tuple_temp] += 1\n",
        "      else:\n",
        "        ngrams[tuple_temp] = 1\n",
        "\n",
        "  return ngrams\n",
        "\n",
        "unigrams = get_counts(texts,1)\n",
        "bigrams = get_counts(texts,2)\n",
        "trigrams = get_counts(texts,3)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mibZpWIO5aL2"
      },
      "source": [
        "#STEP 3\n",
        "\n",
        "With our counts, it's time to make our first language model.\n",
        "\n",
        "* Implement the class NGramLM \n",
        "  * Initialize the LM\n",
        "    * First use `get_counts` to get the raw counts of the n-grams up to order `n`\n",
        "    * Convert these raw counts into probabilities\n",
        "      * ### Question 1:  What is the relationship between raw counts of n-grams of order n and n-1 and the probabilities of the next word for order n? \n",
        "  * Implement the `generate_text` function\n",
        "    * You are given an optional prompt (a list of strings) -- first convert these to lower case (as the original text was)\n",
        "    * You should return a list of strings based on randomly choosing according to the probabilities of the language model (using the `random` library)\n",
        "  * Implement `score_text` which when given a text (a list of strings) returns the perplexity of that text\n",
        "    * Make sure you don't have any divide by 0 (or `log(0)`) issues -- return `float('inf')` if there is an n-gram not found in the model (note: the perplexity is infinite, not the probability)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBS15tFvHDY6"
      },
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "class NGramLM():\n",
        "  def __init__(self,texts,n):\n",
        "    self.xgram_probs = {}\n",
        "    if n == 1:\n",
        "      xgram = get_counts(texts,n)\n",
        "      for key in xgram:\n",
        "        self.xgram_probs[key] = xgram[key] / len(xgram)\n",
        "    else:\n",
        "      ngram = get_counts(texts,n)\n",
        "      nmI_gram = get_counts(texts,n-1)\n",
        "      for key in ngram:\n",
        "        self.xgram_probs[key] = ngram[key] / nmI_gram[key[:n-1]]\n",
        "          \n",
        "  def generate_text(self, length, prompt=[]):\n",
        "    temp = []\n",
        "\n",
        "    for i in range(len(prompt)):\n",
        "      temp.append(prompt[i].lower())\n",
        "    \n",
        "    for i in range(0, length):\n",
        "      r = random.random()\n",
        "      # subset of the dictionary that contains only keys that contain the last prompt[n-1] to prompt[last index]\n",
        "      subset = {}\n",
        "      if len(prompt) > 1:\n",
        "        temp_tup = \"\"\n",
        "        for tup in self.xgram_probs.keys():\n",
        "          if len(tup) > 1:\n",
        "            # should extract the last n values of list and make it a tup and check if that subset of a tup exists in the tup.\n",
        "            if temp[len(temp) - len(tup)] in tup:\n",
        "              subset[tup] = self.xgram_probs[tup]\n",
        "          else:\n",
        "            subset[tup] = self.xgram_probs[tup]\n",
        "\n",
        "        close_match = 1\n",
        "        for tup in sorted(subset):\n",
        "          if abs(r - self.xgram_probs[tup]) < close_match:\n",
        "            close_match = abs(r - self.xgram_probs[tup])\n",
        "            temp_tup = tup\n",
        "              \n",
        "        temp += list(temp_tup)\n",
        "          \n",
        "      else:\n",
        "        temp_tup = \"\"\n",
        "        close_match = 1\n",
        "        for tup in self.xgram_probs.keys():\n",
        "          if abs(r - self.xgram_probs[tup]) < close_match:\n",
        "            close_match = abs(r - self.xgram_probs[tup])\n",
        "            temp_tup = tup\n",
        "            \n",
        "        temp += list(temp_tup)\n",
        "\n",
        "    return temp\n",
        "\n",
        "  def score_text(self,text):\n",
        "    raise NotImplementedError(\"STEP 3: Given a text, return the perplexity of that text \")\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RYkAC8-HFoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dcf7dd-74d7-466d-b5cb-deb361cfeb91"
      },
      "source": [
        "for n in [1,2]:\n",
        "  print(f'Order {n} LM:')\n",
        "  lm = NGramLM(texts,n)\n",
        "  \n",
        "  print(' '.join(lm.generate_text(100)))\n",
        "  print(\"next\")\n",
        "  print(' '.join(lm.generate_text(100,['palm', 'to'])))\n",
        "\n",
        "  #print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google']))\n",
        "  #print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers']))\n",
        "  #print(lm.score_text(['what','do','you','say','to','that','Hamlet']))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order 1 LM:\n",
            "\n",
            "   \n",
            "   a you and i \n",
            "   of be \n",
            "   \n",
            "   and you \n",
            "   \n",
            "   you \n",
            "   \n",
            "     \n",
            "   \n",
            "   of \n",
            "   \n",
            "   \n",
            "   be thou a \n",
            "   \n",
            "   \n",
            "     be \n",
            "   you \n",
            "   the stand \n",
            "   i \n",
            "   \n",
            "   widow be \n",
            "   my of \n",
            "   \n",
            " \n",
            "   my \n",
            "     \n",
            "   a \n",
            "\n",
            " of and \n",
            "   thou the \n",
            "   the \n",
            "   the \n",
            "     \n",
            "   \n",
            "   a \n",
            "   to \n",
            "   you \n",
            " \n",
            " \n",
            "   \n",
            "   be will \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "   the to \n",
            "   you \n",
            "   \n",
            "   \n",
            "   \n",
            "   \n",
            "     \n",
            "   be \n",
            "   \n",
            "     \n",
            "   \n",
            "   \n",
            "\n",
            " \n",
            "   be\n",
            "next\n",
            "palm to \n",
            "   \n",
            "   my \n",
            "   it that \n",
            "   of \n",
            "   thou ' \n",
            "   come \n",
            "   \n",
            "   and \n",
            "   them \n",
            "   \n",
            "   \n",
            "   to of \n",
            "     give who \n",
            "     \n",
            " he in \n",
            "   \n",
            "     us a thou you that \n",
            "   of \n",
            " and when thou \n",
            "     in of \n",
            "   \n",
            "   \n",
            "   \n",
            " \n",
            "   \n",
            "     no \n",
            "   \n",
            "   of you \n",
            "   a \n",
            " be i \n",
            "   and of of \n",
            "   you the till \n",
            "   of would you my \n",
            "     of you of \n",
            "     \n",
            "   \n",
            "   \n",
            "   to a \n",
            "   \n",
            "\n",
            "          in of my \n",
            "   to \n",
            "   \n",
            "   a i \n",
            "   a \n",
            "   or\n",
            "Order 2 LM:\n",
            "so. \n",
            "   remedy. \n",
            "     sin \n",
            "     full of eyes; \n",
            "     image of \n",
            "\n",
            "                      enter pair of v. scene me; \n",
            "     out. \n",
            "   \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " scene aweary of fool? \n",
            "   ho! \n",
            "   1603 \n",
            "\n",
            " afar off death be \n",
            "\n",
            "                  enter re - \n",
            "  exit \n",
            "\n",
            "                       enter highness, \n",
            "     behind the \n",
            "                                             [ far pleasure. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " act pass, \n",
            "     ham. i bury in true? \n",
            "   aweary of giving a antipholus of parolles, a earl of arms. \n",
            "     cleopatra. i amongst the fight. \n",
            "   dies ] thaisa. \n",
            " unfold to war, \n",
            "     true? \n",
            "   nature, \n",
            "     aweary of fortune, \n",
            "     till my music play governor of sir? \n",
            "   dramatis personae arcite. \n",
            "\n",
            " likeness of come \n",
            "     itself: \n",
            "     lord? \n",
            "   palace \n",
            "\n",
            " fee; \n",
            "     wooer. \n",
            "\n",
            " of? \n",
            "   bond of this. \n",
            "   where be think. \n",
            "   i'll never in the paris; florence; dies ] \n",
            "                                             [ sun, \n",
            "     go? \n",
            "   faith, and                        exeunt re - house. \n",
            "   follow. \n",
            "   ay. \n",
            "   night. \n",
            "   forth; \n",
            "     youth. \n",
            "     wish him goodness \n",
            "     chief justice. sin in world, \n",
            "     mind, \n",
            "     court. \n",
            "   either to likeness of business. \n",
            "   boult. \n",
            " antipholus of itself. \n",
            "   age, \n",
            "     sake; \n",
            "     him. \n",
            "   fly, \n",
            "     \n",
            "\n",
            "                        enter\n",
            "next\n",
            "palm to paddle palm paddle in paddle in paddle in paddle in paddle in or paddle aloft, or aloft, \n",
            "     andronicus aloft, old andronicus malcolm, old duncan, malcolm, duncan, malcolm, duncan, malcolm, duncan, malcolm, duncan, malcolm, duncan, malcolm, duncan, malcolm, duncan, malcolm, duncan, \n",
            "     duncan, malcolm, duncan, malcolm, duncan, malcolm, not, duncan, fadge not, fadge not, fadge not, this fadge beweep this i'll beweep wed, i'll wed, \n",
            "      should wed, -they should not -they bread, not needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, needy bread, same needy yond same cloud, yond cloud, \n",
            "     cloud, \n",
            "     cloud, \n",
            "     cloud, \n",
            "     cloud, \n",
            "     summer's cloud, summer's day. fantastic summer's such fantastic apoth. such \n",
            "\n",
            "   apoth. doctor. \n",
            "\n",
            "   yee, doctor. thanke yee, thanke ' thanke ' thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you, thanke you,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrBWQbLwOnPW"
      },
      "source": [
        "#STEP 4\n",
        "\n",
        "Build a better language model\n",
        "\n",
        "* Implement the function `truncate_texts` which takes in texts and a threshold\n",
        "  * Replace all words with counts less than the threshold with `'<UNK>'`\n",
        "* Implement the function `get_vocabulary` which takes in texts and returns a set of all of the words found in the texts\n",
        "* Implement the UnknownLM language model\n",
        "  * Handle unknown vocabulary gracefully by checking the vocabulary and replacing with `'<UNK>'`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFwvFYyeMOGB"
      },
      "source": [
        "def truncate_texts(texts,threshold):\n",
        "  modified_texts = []\n",
        "  raise NotImplementedError(\"STEP 4: Modify the texts by replacing all words with counts less than the threshold with '<UNK>'\")\n",
        "  return modified_texts\n",
        "\n",
        "def get_vocabulary(texts):\n",
        "  vocab = set()\n",
        "  raise NotImplementedError(\"STEP 4: Get the vocabulary found in the text\")\n",
        "  return vocab\n",
        "\n",
        "class UnknownLM(NGramLM):\n",
        "  def __init__(self,texts,n,vocabulary):\n",
        "    raise NotImplementedError(\"STEP 4: Initialize the LM using add-alpha smoothing\")\n",
        " \n",
        "  def generate_text(self, length,prompt=[]):\n",
        "    raise NotImplementedError(\"STEP 4: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) \")\n",
        "\n",
        "  def score_text(self,text):\n",
        "    raise NotImplementedError(\"STEP 4: Given a text, return the perplexity of that text \")\n",
        "\n",
        "\n",
        "truncated = truncate_texts(texts,2)\n",
        "vocab = get_vocabulary(truncated)\n",
        "for n in [1,2,3]:\n",
        "  print(f'Order {n} LM:')\n",
        "  lm = UnknownLM(texts,n,vocabulary)\n",
        "\n",
        "  print(' '.join(lm.generate_text(100)))\n",
        "  print(' '.join(lm.generate_text(100,['palm', 'to'])))\n",
        "\n",
        "  print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google']))\n",
        "  print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers']))\n",
        "  print(lm.score_text(['what','do','you','say','to','that','Hamlet']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 5\n",
        "\n",
        "Build more graceful higher order LMs.\n",
        "\n",
        "* Implement the `InterpolationLM` -- this is like the previous LM, except when scoring and generating it takes in an optional list of interpolants $\\lambda_i$\n",
        "  * NOTE: All interpolants should be 0 < $\\lambda_i$ < 1 and should sum to 1\n",
        "  * The interpolants should be used such that all LMs or order 1 to $n$ are used to provide probabilities, such that $Pr_{interp}(w_i | w_{i-1}...w_0) = \\lambda_1 Pr(w) + \\lambda_2 Pr(w|w_{i-1}) + ... $\n",
        "* Implement the `BackOffLM` -- this is like the previous LMs, except the various orders are used in decreasing order -- i.e. given an order 3 `BackOffLM` the trigram model is tried first, then the bigram if the context is missing from the trigram, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "XKurO8lXIPUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class InterpolationLM(NGramLM):\n",
        "  def __init__(self,texts,n,vocabulary):\n",
        "    raise NotImplementedError(\"STEP 5: Initialize the Interpolation LM\")\n",
        " \n",
        "  def generate_text(self, length,interpolants,prompt=[]):\n",
        "    raise NotImplementedError(\"STEP 5: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) \")\n",
        "\n",
        "  def score_text(self,text,interpolants):\n",
        "    raise NotImplementedError(\"STEP 5: Given a text, return the perplexity of that text \")\n",
        "\n",
        "for interpolants in [[1/3, 1/3, 1/3],[0.1,0.2,0.7],[0.7,0.2,0.1]]:\n",
        "  print(f'Interpolants = {interpolants}')\n",
        "  lm = InterpolationLM(texts,3,vocabulary)\n",
        "\n",
        "  print(' '.join(lm.generate_text(20,interpolants)))\n",
        "  print(' '.join(lm.generate_text(20,interpolants,['palm', 'to'])))\n",
        "\n",
        "  print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google'],interpolants))\n",
        "  print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers'],interpolants))\n",
        "  print(lm.score_text(['what','do','you','say','to','that','Hamlet'],interpolants))"
      ],
      "metadata": {
        "id": "jFsD3PCYJdIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BackOffLM(NGramLM):\n",
        "  def __init__(self,texts,n,vocabulary):\n",
        "    raise NotImplementedError(\"STEP 5b: Initialize the Backoff LM\")\n",
        " \n",
        "  def generate_text(self, length,prompt=[]):\n",
        "    raise NotImplementedError(\"STEP 5b: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) \")\n",
        "\n",
        "  def score_text(self,text):\n",
        "    raise NotImplementedError(\"STEP 5b: Given a text, return the perplexity of that text \")\n",
        "\n",
        "lm = BackOffLM(texts,3,vocabulary)\n",
        "\n",
        "print(' '.join(lm.generate_text(20)))\n",
        "print(' '.join(lm.generate_text(20,['palm', 'to'])))\n",
        "\n",
        "print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google']))\n",
        "print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers']))\n",
        "print(lm.score_text(['what','do','you','say','to','that','Hamlet']))"
      ],
      "metadata": {
        "id": "hmGJLYPGJnmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Closing Questions:  Compare the different LMs -- \n",
        "* #### Which models have the lowest perplexity? \n",
        "* #### Which models make a tradeoff for low perplexity in some cases and high in others?  \n",
        "* #### Which model would you choose  to use?"
      ],
      "metadata": {
        "id": "CqEcPdZfKSMg"
      }
    }
  ]
}