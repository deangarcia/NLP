{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment1_Starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deangarcia/NLP/blob/main/NLP_Assignment1_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoLhgpq8A37v"
      },
      "source": [
        "In this assignment, you will familiarize yourself with:\n",
        "\n",
        "\n",
        "\n",
        "*   Python\n",
        "*   Jupyter Notebooks\n",
        "*   Google Colab\n",
        "\n",
        "to develop\n",
        "\n",
        "* A n-gram based language model, with smoothing\n",
        "\n",
        "to be able to\n",
        "\n",
        "* Produce naturalish text\n",
        "* Rate the perplexity of a text given your model\n",
        "\n",
        "First we will load in some data.\n",
        "\n",
        "Provided is code that will download a file and rename it corpus.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9W83a-6Az4K"
      },
      "source": [
        "!wget -O corpus.zip https://digitalrepository.wheatoncollege.edu/bitstream/handle/11040/24451/Shakespeare%20Corpus%20by%20Play.zip?sequence=1&isAllowed=y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtEkrsKIcfos"
      },
      "source": [
        "By typing \"!\" in a notebook, you can use command line prompts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaqV2dT5C9T5"
      },
      "source": [
        "!ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Ku2ezMBmlm"
      },
      "source": [
        "\n",
        "Now, we will unzip the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19CEka8eDKpq"
      },
      "source": [
        "!unzip -o corpus.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq8mNvQ_D07b"
      },
      "source": [
        "We can even have python and the command line interact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeRC0gLPDPsC"
      },
      "source": [
        "files = !ls Shake*/*/*_noSCRB.txt \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q-QJF9_D7FR"
      },
      "source": [
        "`files` is now a list containing all of the files we wish to use in our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPcAzaBDTei"
      },
      "source": [
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koxj6yTaEkPt"
      },
      "source": [
        "Note, there are extra quotes on the files, so we will use list comprehension to remove those"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Elxvnp8ExLs"
      },
      "source": [
        "files = [file[1:-1] for file in files]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQRR6E_ERL3"
      },
      "source": [
        "#STEP 1\n",
        "\n",
        "\n",
        "* Go through all of the files\n",
        "  * Load the file `file = open(filename,'r')` \n",
        "  * Use the spacy tokenizer to tokenize the text `tokens = tokenizer(file.read())`\n",
        "  * Loop through the tokens and make everything lowercase `.lower()`\n",
        "  * Use all of this to make a list (for each file) of lists of strings (the tokens in each file) called `texts` \n",
        "\n",
        "\n",
        "\n",
        "  e.g.\n",
        "\n",
        "  If we read in one file that looked like:\n",
        "  \n",
        "    HAMLET: Alas poor Yorick.\n",
        "\n",
        "    GRAVEDIGGER: Put that skull down.\n",
        "\n",
        "\n",
        "\n",
        "  The end results should look like:\n",
        "\n",
        "  `texts = [['<s>', 'HAMLET:', 'Alas', 'poor', 'Yorick.','\\n','GRAVEDIGGER:', Put','that', 'skull', 'down.', '</s>']]`\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "1ohoxRLsyKfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDQWXoeNEE5M"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
        "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
        "suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
        "infix_re = re.compile(r'''[-~]''')\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
        "                                prefix_search=prefix_re.search,\n",
        "                                suffix_search=suffix_re.search,\n",
        "                                infix_finditer=infix_re.finditer)\n",
        "    \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "tokenizer = nlp.tokenizer\n",
        "texts = []\n",
        "\n",
        "for filename in files:\n",
        "  test = []\n",
        "  filex = open(filename,'r')\n",
        "  tokens = tokenizer(filex.read())\n",
        "  for token in tokens:\n",
        "    test.append(token.lemma_.lower())\n",
        "  texts.append(test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4jiUQ9QHjRX"
      },
      "source": [
        "#STEP 2\n",
        "\n",
        "With our files read in, it's time to do some counting!\n",
        "\n",
        "* Implement the function `get_counts(texts,n)` which takes in a list of list of strings -- `texts` -- and the arity of the n-gram -- `n` \n",
        "* `get_counts` should return a dictionary where the keys are the n-grams (as tuples of strings) and the values are the integer counts of all of the found n-grams\n",
        "\n",
        "\n",
        "Here's a quick primer on tuples in Python\n",
        "\n",
        "    triple = (1,2,3)\n",
        "    double = (1,2)\n",
        "    single = (1,) #note the trailing comma to distinguish from parenethesis \n",
        "    null_tuple = tuple() \n",
        "\n",
        "    list_triple = [1,2,3]\n",
        "    tuple_triple = tuple(list_triple)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx-MSkDlEeW6"
      },
      "source": [
        "def get_counts(texts,n):\n",
        "  ngrams = {}\n",
        "  for text in texts:\n",
        "    for i in range(len(text) - n + 1):\n",
        "      temp = [] \n",
        "      for j in range(n):\n",
        "        temp.append(text[i+j])\n",
        "      tuple_temp = tuple(temp)\n",
        "      if tuple_temp in ngrams.keys():\n",
        "        ngrams[tuple_temp] += 1\n",
        "      else:\n",
        "        ngrams[tuple_temp] = 1\n",
        "\n",
        "  return ngrams\n",
        "\n",
        "unigrams = get_counts(texts,1)\n",
        "bigrams = get_counts(texts,2)\n",
        "trigrams = get_counts(texts,3)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mibZpWIO5aL2"
      },
      "source": [
        "#STEP 3\n",
        "\n",
        "With our counts, it's time to make our first language model.\n",
        "\n",
        "* Implement the class NGramLM \n",
        "  * Initialize the LM\n",
        "    * First use `get_counts` to get the raw counts of the n-grams up to order `n`\n",
        "    * Convert these raw counts into probabilities\n",
        "      * ### Question 1:  What is the relationship between raw counts of n-grams of order n and n-1 and the probabilities of the next word for order n? \n",
        "  * Implement the `generate_text` function\n",
        "    * You are given an optional prompt (a list of strings) -- first convert these to lower case (as the original text was)\n",
        "    * You should return a list of strings based on randomly choosing according to the probabilities of the language model (using the `random` library)\n",
        "  * Implement `score_text` which when given a text (a list of strings) returns the perplexity of that text\n",
        "    * Make sure you don't have any divide by 0 (or `log(0)`) issues -- return `float('inf')` if there is an n-gram not found in the model (note: the perplexity is infinite, not the probability)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBS15tFvHDY6"
      },
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "class NGramLM():\n",
        "  def __init__(self,texts,n):\n",
        "    self.xgram_probs = {}\n",
        "    if n == 1:\n",
        "      xgram = get_counts(texts,n)\n",
        "      for key in xgram:\n",
        "        self.xgram_probs[key] = xgram[key] / len(xgram)\n",
        "    else:\n",
        "      ngram = get_counts(texts,n)\n",
        "      nmI_gram = get_counts(texts,n-1)\n",
        "      for key in ngram:\n",
        "        self.xgram_probs[key] = ngram[key] / nmI_gram[key[:n-1]]\n",
        "          \n",
        "  def generate_text(self, length, prompt=[]):\n",
        "    temp = []\n",
        "\n",
        "    for i in range(len(prompt)):\n",
        "      temp.append(prompt[i].lower())\n",
        "    \n",
        "    for i in range(0, length):\n",
        "      r = random.random()\n",
        "      # subset of the dictionary that contains only keys that contain the last prompt[n-1] to prompt[last index]\n",
        "      subset = {}\n",
        "      if len(prompt) > 1:\n",
        "        temp_tup = \"\"\n",
        "        for tup in self.xgram_probs.keys():\n",
        "          if len(tup) > 1:\n",
        "            # should extract the last n values of list and make it a tup and check if that subset of a tup exists in the tup.\n",
        "            if temp[len(temp) - len(tup)] in tup:\n",
        "              subset[tup] = self.xgram_probs[tup]\n",
        "          else:\n",
        "            subset[tup] = self.xgram_probs[tup]\n",
        "\n",
        "        close_match = 1\n",
        "        for tup in sorted(subset):\n",
        "          if abs(r - self.xgram_probs[tup]) < close_match:\n",
        "            close_match = abs(r - self.xgram_probs[tup])\n",
        "            temp_tup = tup\n",
        "              \n",
        "        temp += list(temp_tup)\n",
        "          \n",
        "      else:\n",
        "        temp_tup = \"\"\n",
        "        close_match = 1\n",
        "        for tup in self.xgram_probs.keys():\n",
        "          if abs(r - self.xgram_probs[tup]) < close_match:\n",
        "            close_match = abs(r - self.xgram_probs[tup])\n",
        "            temp_tup = tup\n",
        "            \n",
        "        temp += list(temp_tup)\n",
        "\n",
        "    return temp\n",
        "\n",
        "  def score_text(self,text):\n",
        "    raise NotImplementedError(\"STEP 3: Given a text, return the perplexity of that text \")\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RYkAC8-HFoo"
      },
      "source": [
        "for n in [1,2]:\n",
        "  print(f'Order {n} LM:')\n",
        "  lm = NGramLM(texts,n)\n",
        "  \n",
        "  print(' '.join(lm.generate_text(100)))\n",
        "  print(\"next\")\n",
        "  print(' '.join(lm.generate_text(100,['palm', 'to'])))\n",
        "\n",
        "  print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google']))\n",
        "  print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers']))\n",
        "  print(lm.score_text(['what','do','you','say','to','that','Hamlet']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrBWQbLwOnPW"
      },
      "source": [
        "#STEP 4\n",
        "\n",
        "Build a better language model\n",
        "\n",
        "* Implement the function `truncate_texts` which takes in texts and a threshold\n",
        "  * Replace all words with counts less than the threshold with `'<UNK>'`\n",
        "* Implement the function `get_vocabulary` which takes in texts and returns a set of all of the words found in the texts\n",
        "* Implement the UnknownLM language model\n",
        "  * Handle unknown vocabulary gracefully by checking the vocabulary and replacing with `'<UNK>'`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFwvFYyeMOGB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "b0ba31b1-a48e-4b09-9457-13a44e221e64"
      },
      "source": [
        "def truncate_texts(texts,threshold):\n",
        "  modified_texts = []\n",
        "  for text in texts:\n",
        "    for word in text:\n",
        "      if len(word) > threshold:\n",
        "        modified_texts.append(word)\n",
        "      else:\n",
        "        modified_texts.append(\"<UNK>\")\n",
        "  return modified_texts\n",
        "\n",
        "def get_vocabulary(texts):\n",
        "  # does this vocab mean I should take out punctuation and /ns numbers ect?\n",
        "  vocab = set()\n",
        "  for text in texts:\n",
        "    for word in text:\n",
        "      vocab.add(word)\n",
        "  return vocab\n",
        "\n",
        "class UnknownLM(NGramLM):\n",
        "  def __init__(self,texts,n,vocabulary):\n",
        "    raise NotImplementedError(\"STEP 4: Initialize the LM using add-alpha smoothing\")\n",
        " \n",
        "  def generate_text(self, length,prompt=[]):\n",
        "    raise NotImplementedError(\"STEP 4: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) \")\n",
        "\n",
        "  def score_text(self,text):\n",
        "    raise NotImplementedError(\"STEP 4: Given a text, return the perplexity of that text \")\n",
        "\n",
        "\n",
        "truncated = truncate_texts(texts,2)\n",
        "vocab = get_vocabulary(truncated)\n",
        "for n in [1,2,3]:\n",
        "  print(f'Order {n} LM:')\n",
        "  lm = UnknownLM(texts,n,vocab)\n",
        "\n",
        "  print(' '.join(lm.generate_text(100)))\n",
        "  print(' '.join(lm.generate_text(100,['palm', 'to'])))\n",
        "\n",
        "  print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google']))\n",
        "  print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers']))\n",
        "  print(lm.score_text(['what','do','you','say','to','that','Hamlet']))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order 1 LM:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-bfa2e2320319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnknownLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'palm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-168-bfa2e2320319>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(self, length, prompt)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STEP 4: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mscore_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: STEP 4: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 5\n",
        "\n",
        "Build more graceful higher order LMs.\n",
        "\n",
        "* Implement the `InterpolationLM` -- this is like the previous LM, except when scoring and generating it takes in an optional list of interpolants $\\lambda_i$\n",
        "  * NOTE: All interpolants should be 0 < $\\lambda_i$ < 1 and should sum to 1\n",
        "  * The interpolants should be used such that all LMs or order 1 to $n$ are used to provide probabilities, such that $Pr_{interp}(w_i | w_{i-1}...w_0) = \\lambda_1 Pr(w) + \\lambda_2 Pr(w|w_{i-1}) + ... $\n",
        "* Implement the `BackOffLM` -- this is like the previous LMs, except the various orders are used in decreasing order -- i.e. given an order 3 `BackOffLM` the trigram model is tried first, then the bigram if the context is missing from the trigram, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "XKurO8lXIPUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class InterpolationLM(NGramLM):\n",
        "  def __init__(self,texts,n,vocabulary):\n",
        "    raise NotImplementedError(\"STEP 5: Initialize the Interpolation LM\")\n",
        " \n",
        "  def generate_text(self, length,interpolants,prompt=[]):\n",
        "    raise NotImplementedError(\"STEP 5: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) \")\n",
        "\n",
        "  def score_text(self,text,interpolants):\n",
        "    raise NotImplementedError(\"STEP 5: Given a text, return the perplexity of that text \")\n",
        "\n",
        "for interpolants in [[1/3, 1/3, 1/3],[0.1,0.2,0.7],[0.7,0.2,0.1]]:\n",
        "  print(f'Interpolants = {interpolants}')\n",
        "  lm = InterpolationLM(texts,3,vocabulary)\n",
        "\n",
        "  print(' '.join(lm.generate_text(20,interpolants)))\n",
        "  print(' '.join(lm.generate_text(20,interpolants,['palm', 'to'])))\n",
        "\n",
        "  print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google'],interpolants))\n",
        "  print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers'],interpolants))\n",
        "  print(lm.score_text(['what','do','you','say','to','that','Hamlet'],interpolants))"
      ],
      "metadata": {
        "id": "jFsD3PCYJdIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BackOffLM(NGramLM):\n",
        "  def __init__(self,texts,n,vocabulary):\n",
        "    raise NotImplementedError(\"STEP 5b: Initialize the Backoff LM\")\n",
        " \n",
        "  def generate_text(self, length,prompt=[]):\n",
        "    raise NotImplementedError(\"STEP 5b: Given a prompt (a list of strings) generate length number of strings -- return a list containing all of the text (prompt + generated) \")\n",
        "\n",
        "  def score_text(self,text):\n",
        "    raise NotImplementedError(\"STEP 5b: Given a text, return the perplexity of that text \")\n",
        "\n",
        "lm = BackOffLM(texts,3,vocabulary)\n",
        "\n",
        "print(' '.join(lm.generate_text(20)))\n",
        "print(' '.join(lm.generate_text(20,['palm', 'to'])))\n",
        "\n",
        "print(lm.score_text(['IBM','announced','today','that','they','will','be','buying','Google']))\n",
        "print(lm.score_text(['palm', 'to', 'palm', 'is', 'holy', 'palmers']))\n",
        "print(lm.score_text(['what','do','you','say','to','that','Hamlet']))"
      ],
      "metadata": {
        "id": "hmGJLYPGJnmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Closing Questions:  Compare the different LMs -- \n",
        "* #### Which models have the lowest perplexity? \n",
        "* #### Which models make a tradeoff for low perplexity in some cases and high in others?  \n",
        "* #### Which model would you choose  to use?"
      ],
      "metadata": {
        "id": "CqEcPdZfKSMg"
      }
    }
  ]
}